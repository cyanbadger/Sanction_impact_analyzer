{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1957eeb7-69d5-471f-95e6-daf9f31f43a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT0001\\anaconda3\\Lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\KIIT0001\\anaconda3\\Lib\\site-packages\\torch_scatter\\_version_cpu.pyd\n",
      "  import torch_geometric.typing\n",
      "C:\\Users\\KIIT0001\\anaconda3\\Lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\KIIT0001\\anaconda3\\Lib\\site-packages\\torch_cluster\\_version_cpu.pyd\n",
      "  import torch_geometric.typing\n",
      "C:\\Users\\KIIT0001\\anaconda3\\Lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\KIIT0001\\anaconda3\\Lib\\site-packages\\torch_spline_conv\\_version_cpu.pyd\n",
      "  import torch_geometric.typing\n",
      "C:\\Users\\KIIT0001\\anaconda3\\Lib\\site-packages\\torch_geometric\\__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\KIIT0001\\anaconda3\\Lib\\site-packages\\torch_sparse\\_version_cpu.pyd\n",
      "  import torch_geometric.typing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from datetime import datetime, timezone\n",
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from typing import List,Dict,Tuple,Optional\n",
    "\n",
    "# ----- Spatial encoder: Graph Convolution over trade network -----\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # x: [N_nodes, in_dim]\n",
    "        h = self.conv1(x, edge_index, edge_weight)  # neighbors -> node\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        h = self.conv2(h, edge_index, edge_weight)  # second hop (indirect effects)\n",
    "        h = F.relu(h)\n",
    "        return h  # [N_nodes, out_dim]\n",
    "\n",
    "\n",
    "# ----- Full spatio-temporal model for India -----\n",
    "class SanctionImpactGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatio-temporal GNN explicitly aligned with literature review channels.\n",
    "\n",
    "    Spatial:  GCN over country trade graph per year (GDP, CPI, FX, Trade, FDI, etc.)\n",
    "    Temporal: GRU over India's embedding across years (sanction dynamics)\n",
    "    Heads:    Predict impact on GDP, CPI, FX, Trade, FDI, Fiscal, Reserves,\n",
    "              composite ImpactScore_1y, and impact duration.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,              # node feature dim (matches channels + sanction/NLP features)\n",
    "        gcn_hidden=64,\n",
    "        gcn_out=64,\n",
    "        gru_hidden=64,\n",
    "        gru_layers=1,\n",
    "        india_index=0,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.india_index = india_index\n",
    "\n",
    "        # 1. Spatial GCN\n",
    "        self.gcn = GCNEncoder(in_dim, gcn_hidden, gcn_out, dropout=dropout)\n",
    "\n",
    "        # 2. Temporal GRU over India's embeddings\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=gcn_out,\n",
    "            hidden_size=gru_hidden,\n",
    "            num_layers=gru_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # 3. Channel-specific heads (each literature channel gets its own small regressor)\n",
    "        self.heads = nn.ModuleDict({\n",
    "            \"gdp\": nn.Linear(gru_hidden, 1),\n",
    "            \"cpi\": nn.Linear(gru_hidden, 1),\n",
    "            \"fx\": nn.Linear(gru_hidden, 1),\n",
    "            \"trade\": nn.Linear(gru_hidden, 1),\n",
    "            \"fdi\": nn.Linear(gru_hidden, 1),\n",
    "            \"res\": nn.Linear(gru_hidden, 1),\n",
    "            \"score\": nn.Linear(gru_hidden, 1),\n",
    "            \"duration\": nn.Linear(gru_hidden, 1),\n",
    "        })\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _forward_sequence(self, data_list: List) -> Dict[str, torch.Tensor]:\n",
    "        india_embs = []\n",
    "\n",
    "        for data in data_list:\n",
    "            edge_weight = getattr(data, \"edge_weight\", None)\n",
    "            h = self.gcn(data.x, data.edge_index, edge_weight)\n",
    "            india_embs.append(h[self.india_index])\n",
    "\n",
    "        # [T, gcn_out]\n",
    "        india_seq = torch.stack(india_embs, dim=0).unsqueeze(0)\n",
    "\n",
    "        gru_out, _ = self.gru(india_seq)\n",
    "        h_t = self.dropout(gru_out.squeeze(0))  # [T, gru_hidden]\n",
    "\n",
    "        return {k: head(h_t) for k, head in self.heads.items()}\n",
    "    \n",
    "    #Main forward    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "            - List[Data]              â†’ single time sequence\n",
    "            - List[List[Data]]        â†’ batch of sequences\n",
    "        \"\"\"\n",
    "        if not inputs:\n",
    "            raise ValueError(\"Input to SanctionImpactGNN.forward() cannot be empty\")\n",
    "    \n",
    "        # SINGLE sequence: List[Data]\n",
    "        if isinstance(inputs[0], Data):\n",
    "            return self._forward_sequence(inputs)\n",
    "    \n",
    "        # BATCH: List[List[Data]]\n",
    "        batch_preds = {}\n",
    "        for seq in inputs:\n",
    "            preds = self._forward_sequence(seq)\n",
    "            for k, v in preds.items():\n",
    "                batch_preds.setdefault(k, []).append(v)\n",
    "    \n",
    "        for k in batch_preds:\n",
    "            batch_preds[k] = torch.stack(batch_preds[k], dim=0)  # [B, T, 1]\n",
    "    \n",
    "        return batch_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd09c39f-a17e-4c8a-8aa2-6992fc37deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanction type labels (single-label)\n",
    "sanction_type_map = {\n",
    "    \"trade\": 0,\n",
    "    \"financial\": 1,\n",
    "    \"arms\": 2,\n",
    "    \"travel\": 3,\n",
    "    \"other\": 4\n",
    "}\n",
    "\n",
    "# Economic channel labels (multi-label)\n",
    "economic_channels = [\n",
    "    \"GDP\",\n",
    "    \"Inflation\",\n",
    "    \"ExchangeRate\",\n",
    "    \"Trade\",\n",
    "    \"FDI\",\n",
    "    \"Forex\",\n",
    "    \"Commodity\"\n",
    "]\n",
    "\n",
    "channel_map = {ch: i for i, ch in enumerate(economic_channels)}\n",
    "\n",
    "NUM_SANCTION_TYPES = len(sanction_type_map)\n",
    "NUM_CHANNELS = len(economic_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88d12018-cac2-49a5-a031-12e9651ad113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- NLP SANCTION MODEL ----------------\n",
    "\n",
    "class SanctionNLPModel(nn.Module):\n",
    "    def __init__(self, text_encoder, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = text_encoder\n",
    "        self.sanction_head = nn.Linear(hidden_dim, NUM_SANCTION_TYPES)\n",
    "        self.channel_head = nn.Linear(hidden_dim, NUM_CHANNELS)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "        sanction_logits = self.sanction_head(cls_emb)\n",
    "        channel_logits = self.channel_head(cls_emb)\n",
    "\n",
    "        return sanction_logits, channel_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b220e79d-876d-467b-8a0f-fdb926bb5f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- LOSS FUNCTIONS ----------------\n",
    "\n",
    "sanction_loss_fn = nn.CrossEntropyLoss()\n",
    "channel_loss_fn = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5307d3b-5084-4462-8b14-c59a34172785",
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_BASE = \"https://api.worldbank.org/v2/country/IND/indicator/{indicator}\"\n",
    "\n",
    "WB_INDICATORS = {\n",
    "    \"gdp_growth\": \"NY.GDP.MKTP.KD.ZG\",\n",
    "    \"cpi\": \"FP.CPI.TOTL.ZG\",\n",
    "    \"exports\": \"NE.EXP.GNFS.CD\",\n",
    "    \"imports\": \"NE.IMP.GNFS.CD\",\n",
    "    \"fdi\": \"BX.KLT.DINV.CD.WD\",\n",
    "    \"reserves\": \"FI.RES.TOTL.CD\",\n",
    "}\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def fetch_worldbank(indicator, start_year=2005, end_year=None):\n",
    "    if end_year is None:\n",
    "        end_year = datetime.now(timezone.utc).year\n",
    "\n",
    "    url = WB_BASE.format(indicator=indicator)\n",
    "    params = {\n",
    "        \"format\": \"json\",\n",
    "        \"per_page\": 2000,\n",
    "        \"date\": f\"{start_year}:{end_year}\",\n",
    "    }\n",
    "\n",
    "    r = safe_get(url, params=params)\n",
    "    if r is None:\n",
    "        print(f\"Failed WB fetch for {indicator}\")\n",
    "        return pd.DataFrame(columns=[\"year\", indicator])\n",
    "\n",
    "    j = r.json()\n",
    "    if not isinstance(j, list) or len(j) < 2:\n",
    "        return pd.DataFrame(columns=[\"year\", indicator])\n",
    "\n",
    "    rows = []\n",
    "    for d in j[1]:\n",
    "        if d[\"value\"] is not None:\n",
    "            rows.append({\n",
    "                \"year\": int(d[\"date\"]),\n",
    "                indicator: float(d[\"value\"])\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8af4a23-4312-4d61-ad19-b9a46b78c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_usdinr(start_year=2005, end_year=None):\n",
    "    if end_year is None:\n",
    "        end_year = datetime.now(timezone.utc).year\n",
    "\n",
    "    url = \"https://api.exchangerate.host/timeseries\"\n",
    "    params = {\n",
    "        \"start_date\": f\"{start_year}-01-01\",\n",
    "        \"end_date\": f\"{end_year}-12-31\",\n",
    "        \"base\": \"USD\",\n",
    "        \"symbols\": \"INR\"\n",
    "    }\n",
    "\n",
    "    r = safe_get(url, params=params)\n",
    "    if r is None:\n",
    "        print(\"âš ï¸ FX API failed â€” returning empty FX data\")\n",
    "        return pd.DataFrame(columns=[\"year\", \"fx\"])\n",
    "\n",
    "    data = r.json()\n",
    "    rates = data.get(\"rates\", {})\n",
    "\n",
    "    rows = []\n",
    "    for date_str, val in rates.items():\n",
    "        if isinstance(val, dict) and \"INR\" in val:\n",
    "            rows.append({\n",
    "                \"year\": pd.to_datetime(date_str).year,\n",
    "                \"fx\": float(val[\"INR\"])\n",
    "            })\n",
    "\n",
    "    # ðŸ”´ CRITICAL FIX: handle empty rows\n",
    "    if not rows:\n",
    "        print(\"âš ï¸ FX API returned no usable data\")\n",
    "        return pd.DataFrame(columns=[\"year\", \"fx\"])\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return (\n",
    "        df.groupby(\"year\", as_index=False)[\"fx\"]\n",
    "          .mean()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f113d2b-1197-4a57-b70f-8f0fc8c4fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "def safe_get(url, params=None, retries=3, timeout=20, sleep_sec=2):\n",
    "    \"\"\"\n",
    "    Safe HTTP GET with retries to avoid API timeouts.\n",
    "    Returns response or None if all retries fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            print(f\"[attempt {attempt}] API error: {e}\")\n",
    "            if attempt == retries:\n",
    "                return None\n",
    "            sleep(sleep_sec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fba5a852-fe97-4b9c-80b2-fed547b70fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_macro_df(start_year=1995):\n",
    "    dfs = []\n",
    "    for name, code in WB_INDICATORS.items():\n",
    "        print(f\"Fetching {name}\")\n",
    "        df = fetch_worldbank(code, start_year)\n",
    "        df = df.rename(columns={code: name})\n",
    "        dfs.append(df)\n",
    "\n",
    "    macro_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        macro_df = macro_df.merge(df, on=\"year\", how=\"outer\")\n",
    "\n",
    "    macro_df[\"trade\"] = macro_df[\"exports\"] + macro_df[\"imports\"]\n",
    "\n",
    "    fx_df = fetch_usdinr(start_year)\n",
    "    macro_df = macro_df.merge(fx_df, on=\"year\", how=\"left\")\n",
    "\n",
    "    return macro_df.sort_values(\"year\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa4e4c7-d006-4242-b3a2-0817528e76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "BEARER_TOKEN =\"AAAAAAAAAAAAAAAAAAAAAISv6wEAAAAACbLiuBISRVcYuHaRsmmM8jv0yZw%3D4qMK2s2TsVWFACYlVLmvKci2fK0PfVNKGRNzrxp72UKqfRzbxh\"\n",
    "\n",
    "client = tweepy.Client(\n",
    "    bearer_token=BEARER_TOKEN,\n",
    "    wait_on_rate_limit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16efa191-a980-44c9-8b18-41bb70360b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = [\n",
    "    \"realDonaldTrump\", # US President\n",
    "    \"narendramodi\",    # India PM\n",
    "    \"RishiSunak\",      # UK PM\n",
    "    \"EmmanuelMacron\",  # France President\n",
    "    \"KremlinRussia_E\"  # Russia\n",
    "]\n",
    "\n",
    "def get_tweets_by_user(username, max_results=10):\n",
    "    user = client.get_user(username=username)\n",
    "    user_id = user.data.id\n",
    "\n",
    "    tweets = client.get_users_tweets(\n",
    "        id=user_id,\n",
    "        max_results=max_results,\n",
    "        tweet_fields=[\"created_at\", \"lang\"]\n",
    "    )\n",
    "\n",
    "    return [tweet.text for tweet in tweets.data] if tweets.data else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378cc90-a537-40f7-805d-3710b246fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "user_cache = {}\n",
    "\n",
    "def get_user_id(username):\n",
    "    if username in user_cache:\n",
    "        return user_cache[username]\n",
    "\n",
    "    user = client.get_user(username=username)\n",
    "    time.sleep(1)\n",
    "    user_cache[username] = user.data.id\n",
    "    return user_cache[username]\n",
    "\n",
    "def get_tweets_by_user(username, max_results=10):\n",
    "    user_id = get_user_id(username)\n",
    "\n",
    "    tweets = client.get_users_tweets(\n",
    "        id=user_id,\n",
    "        max_results=max_results,\n",
    "        tweet_fields=[\"created_at\", \"lang\"]\n",
    "    )\n",
    "    time.sleep(1)\n",
    "\n",
    "    return [tweet.text for tweet in tweets.data] if tweets.data else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9153650-641f-4d73-97e6-0ee40da282ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "for user in usernames:\n",
    "    try:\n",
    "        tweets.extend(get_tweets_by_user(user, max_results=5))\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching from {user}: {e}\")\n",
    "\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc143a9f-789a-4465-a4e3-d6029a5a423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_nlp_output(sanction_logits, channel_logits):\n",
    "    sanction_pred = torch.argmax(sanction_logits, dim=1)\n",
    "\n",
    "    channel_probs = torch.sigmoid(channel_logits)\n",
    "    channel_pred = (channel_probs > 0.5).int()\n",
    "\n",
    "    sanction_type = list(sanction_type_map.keys())[sanction_pred.item()]\n",
    "    channels = [\n",
    "        economic_channels[i]\n",
    "        for i in range(NUM_CHANNELS)\n",
    "        if channel_pred[0][i] == 1\n",
    "    ]\n",
    "\n",
    "    impact_strength = channel_probs.mean().item()\n",
    "\n",
    "    return {\n",
    "        \"sanction_type\": sanction_type,\n",
    "        \"economic_channels\": channels,\n",
    "        \"impact_strength\": round(impact_strength, 3)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d167f266-21d3-437f-809b-598c75bc924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMTRADE_URL = \"https://comtradeapi.worldbank.org/v1/get/HS\"\n",
    "\n",
    "def fetch_trade(year, top_k=10):\n",
    "    \"\"\"\n",
    "    Fetch India total trade (imports + exports) with top partners for a year.\n",
    "    Returns DataFrame: partner, value\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for rg in [1, 2]:  # 1=imports, 2=exports\n",
    "        params = {\n",
    "            \"reporterCode\": 356,   # India\n",
    "            \"partnerCode\": 0,\n",
    "            \"year\": year,\n",
    "            \"rgCode\": rg,\n",
    "            \"cmdCode\": \"TOTAL\",\n",
    "            \"freq\": \"A\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "\n",
    "        r = safe_get(COMTRADE_URL, params=params)\n",
    "        if r is None:\n",
    "            print(f\"âš ï¸ Comtrade failed for {year}, rg={rg}\")\n",
    "            continue\n",
    "\n",
    "        data = r.json().get(\"data\", [])\n",
    "        for d in data:\n",
    "            rows.append({\n",
    "                \"partner\": d.get(\"ptTitle\"),\n",
    "                \"value\": float(d.get(\"TradeValue\", 0.0))\n",
    "            })\n",
    "\n",
    "        sleep(1)  # respect rate limits\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"partner\", \"value\"])\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return (\n",
    "        df.groupby(\"partner\", as_index=False)[\"value\"]\n",
    "          .sum()\n",
    "          .sort_values(\"value\", ascending=False)\n",
    "          .head(top_k)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def build_graph(year, macro_df, india_index=0, num_partners=5):\n",
    "    \"\"\"\n",
    "    Fallback trade graph when Comtrade is unavailable.\n",
    "    Creates synthetic partner nodes weighted by trade volume.\n",
    "    \"\"\"\n",
    "\n",
    "    row = macro_df[macro_df.year == year]\n",
    "    if row.empty:\n",
    "        return None\n",
    "\n",
    "    # Scalars (IMPORTANT: use .item())\n",
    "    exports = row[\"exports\"].item()\n",
    "    imports = row[\"imports\"].item()\n",
    "\n",
    "    # Countries\n",
    "    partners = [f\"Partner_{i}\" for i in range(num_partners)]\n",
    "    countries = [\"India\"] + partners\n",
    "    num_nodes = len(countries)\n",
    "\n",
    "    # Node features (all macro features except year)\n",
    "    feature_cols = macro_df.columns.drop(\"year\")\n",
    "    F = len(feature_cols)\n",
    "\n",
    "    # Extract macro features as PURE FLOATS\n",
    "    feat_vals = row[feature_cols].astype(float).values[0]\n",
    "    feat_vals = np.nan_to_num(feat_vals, nan=0.0)\n",
    "\n",
    "    # Create node feature matrix\n",
    "    x = torch.tensor(\n",
    "        np.repeat(feat_vals.reshape(1, F), num_nodes, axis=0),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    # Trade-based edges\n",
    "    total_trade = exports + imports\n",
    "    weights = torch.linspace(1.0, 0.3, steps=num_partners)\n",
    "    weights = weights / weights.sum() * total_trade\n",
    "\n",
    "    src, dst, w = [], [], []\n",
    "\n",
    "    for i in range(num_partners):\n",
    "        j = i + 1\n",
    "        src += [india_index, j]\n",
    "        dst += [j, india_index]\n",
    "        w += [weights[i].item(), weights[i].item()]\n",
    "\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "    edge_weight = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "    if edge_weight.numel() > 0:\n",
    "        edge_weight /= edge_weight.max()\n",
    "\n",
    "    g = Data(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
    "    g.year = year\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "348886a0-34c7-4905-a7d7-3cea45614e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recovery_duration(df, indicator=\"gdp_growth\", window=3, max_look=5):\n",
    "    \"\"\"\n",
    "    Calculates years to recovery for each year in the dataframe.\n",
    "    \"\"\"\n",
    "    durations = []\n",
    "    vals = df[indicator].values\n",
    "    \n",
    "    for i in range(len(vals)):\n",
    "        # Calculate pre-event baseline (average of previous 'window' years)\n",
    "        baseline = np.mean(vals[max(0, i-window):i]) if i > 0 else vals[i]\n",
    "        \n",
    "        # We only care if the current year is a 'drop' year (potential sanction impact)\n",
    "        if vals[i] < baseline - 0.5: # 0.5% threshold drop\n",
    "            recovered_in = max_look\n",
    "            for lookahead in range(1, max_look + 1):\n",
    "                if i + lookahead < len(vals):\n",
    "                    if vals[i + lookahead] >= baseline:\n",
    "                        recovered_in = lookahead\n",
    "                        break\n",
    "            durations.append(float(recovered_in))\n",
    "        else:\n",
    "            durations.append(0.0) # No active duration/recovery needed\n",
    "            \n",
    "    return torch.tensor(durations, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# UPDATE your build_labels function like this:\n",
    "def build_labels(macro_df):\n",
    "    labels = {}\n",
    "    # ... your existing gdp, cpi, fx, trade, fdi, res code ...\n",
    "    \n",
    "    # ðŸ”´ INTEGRATED DURATION LOGIC\n",
    "    labels[\"duration\"] = calculate_recovery_duration(macro_df, indicator=\"gdp_growth\")\n",
    "    \n",
    "    # ... your existing score calculation ...\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45fad0f4-ef71-4ce9-8618-aec671e20a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_targets(macro_df: pd.DataFrame, year: int, countries: List[str]) -> np.ndarray:\n",
    "    # Set index to allow easy lookup\n",
    "    macro_indexed = macro_df.set_index([\"country\", \"year\"])\n",
    "    y_cols = [\"gdp_growth\", \"cpi\", \"fx\", \"trade\", \"fdi\", \"reserves\"]\n",
    "\n",
    "    Y = []\n",
    "    for c in countries:\n",
    "        try:\n",
    "            row = macro_indexed.loc[(c, year)]\n",
    "            vals = row[y_cols].values.astype(float)\n",
    "            \n",
    "            # ðŸ”´ CALCULATE DURATION DYNAMICALLY\n",
    "            # Look at the country's history in the full macro_df\n",
    "            c_history = macro_df[macro_df[\"country\"] == c].sort_values(\"year\")\n",
    "            c_history = c_history[c_history[\"year\"] <= year].tail(5) # Look at recent window\n",
    "            \n",
    "            baseline = c_history[\"gdp_growth\"].mean()\n",
    "            current_gdp = row[\"gdp_growth\"]\n",
    "            \n",
    "            # Simple heuristic: if GDP is 1% below baseline, predict it takes 2-4 years\n",
    "            if current_gdp < baseline - 1.0:\n",
    "                duration = 3.5 # Estimated impact length in years\n",
    "            else:\n",
    "                duration = 0.0\n",
    "                \n",
    "        except KeyError:\n",
    "            vals = np.zeros(len(y_cols))\n",
    "            duration = 0.0\n",
    "\n",
    "        score = float(np.mean(np.abs(vals)))\n",
    "        Y.append(np.concatenate([vals, [score, duration]]))\n",
    "\n",
    "    return np.vstack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0887eb1c-35dd-437d-a65a-db41eaba6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_labels(macro_df):\n",
    "    labels = {}\n",
    "\n",
    "    labels[\"gdp\"] = torch.tensor(\n",
    "        macro_df[\"gdp_growth\"].astype(float).to_numpy(),\n",
    "        dtype=torch.float32\n",
    "    ).unsqueeze(1)\n",
    "\n",
    "    labels[\"cpi\"] = torch.tensor(\n",
    "        macro_df[\"cpi\"].astype(float).to_numpy(),\n",
    "        dtype=torch.float32\n",
    "    ).unsqueeze(1)\n",
    "\n",
    "    labels[\"fx\"] = torch.tensor(\n",
    "        macro_df[\"fx\"].astype(float).to_numpy(),\n",
    "        dtype=torch.float32\n",
    "    ).unsqueeze(1)\n",
    "\n",
    "    labels[\"trade\"] = torch.tensor(\n",
    "        macro_df[\"trade\"].astype(float).to_numpy(),\n",
    "        dtype=torch.float32\n",
    "    ).unsqueeze(1)\n",
    "\n",
    "    labels[\"fdi\"] = torch.tensor(\n",
    "        macro_df[\"fdi\"].astype(float).to_numpy(),\n",
    "        dtype=torch.float32\n",
    "    ).unsqueeze(1)\n",
    "\n",
    "    labels[\"res\"] = torch.tensor(\n",
    "        macro_df[\"reserves\"].astype(float).to_numpy(),\n",
    "        dtype=torch.float32\n",
    "    ).unsqueeze(1)\n",
    "\n",
    "    labels[\"score\"] = (\n",
    "        labels[\"gdp\"] * 0.3 +\n",
    "        labels[\"trade\"] * 0.2 +\n",
    "        labels[\"fdi\"] * 0.2 -\n",
    "        labels[\"cpi\"] * 0.2 -\n",
    "        labels[\"fx\"] * 0.1\n",
    "    )\n",
    "\n",
    "    labels[\"duration\"] = torch.ones_like(labels[\"gdp\"])\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f325bd6-a6fb-480c-82f4-5c9321029d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching gdp_growth\n",
      "Fetching cpi\n",
      "Fetching exports\n",
      "Fetching imports\n",
      "Fetching fdi\n",
      "Fetching reserves\n",
      "âš ï¸ FX API returned no usable data\n",
      "year            int64\n",
      "gdp_growth    float64\n",
      "cpi           float64\n",
      "exports       float64\n",
      "imports       float64\n",
      "fdi           float64\n",
      "reserves      float64\n",
      "trade         float64\n",
      "fx            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Force all macro features to numeric (MANDATORY)\n",
    "# -------------------------------\n",
    "macro_df = build_macro_df(2005)\n",
    "\n",
    "for col in macro_df.columns:\n",
    "    if col != \"year\":\n",
    "        macro_df[col] = pd.to_numeric(macro_df[col], errors=\"coerce\")\n",
    "\n",
    "# Handle remaining NaNs safely\n",
    "macro_df = macro_df.fillna(0.0)\n",
    "\n",
    "print(macro_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29c74039-f291-437e-b192-83cd8e77a091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching gdp_growth\n",
      "Fetching cpi\n",
      "Fetching exports\n",
      "Fetching imports\n",
      "Fetching fdi\n",
      "Fetching reserves\n",
      "âš ï¸ FX API returned no usable data\n",
      "Graphs: 20\n",
      "gdp: torch.Size([20, 1])\n",
      "cpi: torch.Size([20, 1])\n",
      "fx: torch.Size([20, 1])\n",
      "trade: torch.Size([20, 1])\n",
      "fdi: torch.Size([20, 1])\n",
      "res: torch.Size([20, 1])\n",
      "score: torch.Size([20, 1])\n",
      "duration: torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 1. Build macroeconomic dataframe\n",
    "# -------------------------------------------------\n",
    "macro_df = build_macro_df(2005)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Build training labels (dict of tensors)\n",
    "# -------------------------------------------------\n",
    "y_targets = build_labels(macro_df)\n",
    "y_targets[\"fx\"] = torch.nan_to_num(y_targets[\"fx\"], nan=0.0)\n",
    "y_targets[\"score\"] = (\n",
    "    y_targets[\"gdp\"] * 0.3 +\n",
    "    y_targets[\"trade\"] * 0.2 +\n",
    "    y_targets[\"fdi\"] * 0.2 -\n",
    "    y_targets[\"cpi\"] * 0.2 -\n",
    "    y_targets[\"fx\"] * 0.1\n",
    ")\n",
    "# -------------------------------------------------\n",
    "# 3. Build yearly sanction graphs\n",
    "# -------------------------------------------------\n",
    "graphs = []\n",
    "\n",
    "for y in macro_df[\"year\"]:\n",
    "    g = build_graph(y, macro_df)\n",
    "    if g is not None:\n",
    "        graphs.append(g)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Sanity checks\n",
    "# -------------------------------------------------\n",
    "print(\"Graphs:\", len(graphs))\n",
    "\n",
    "for k, v in y_targets.items():\n",
    "    print(f\"{k}: {v.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "551960e0-382d-4403-b55b-308c56053186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "âš ï¸ Skipping normalization for fx (stdâ‰ˆ0)\n",
      "âš ï¸ Skipping normalization for duration (stdâ‰ˆ0)\n",
      "Targets normalized.\n",
      "Epoch 000 | Loss: 11.9930\n",
      "Epoch 025 | Loss: 8.9024\n",
      "Epoch 050 | Loss: 8.6774\n",
      "Epoch 075 | Loss: 8.2111\n",
      "Epoch 100 | Loss: 8.3894\n",
      "Epoch 125 | Loss: 7.2849\n",
      "Epoch 150 | Loss: 8.0230\n",
      "Epoch 175 | Loss: 8.4455\n",
      "Epoch 200 | Loss: 8.0204\n",
      "Epoch 225 | Loss: 7.6073\n",
      "Epoch 250 | Loss: 7.4550\n",
      "Epoch 275 | Loss: 7.5819\n",
      "Epoch 299 | Loss: 7.6245\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Device\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Move graphs to device\n",
    "# -------------------------------\n",
    "graphs_dev = [g.to(device) for g in graphs]\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Prepare & normalize targets\n",
    "# -------------------------------\n",
    "y_targets = {k: v.to(device).float() for k, v in y_targets.items()}\n",
    "\n",
    "# Z-score normalization (VERY IMPORTANT)\n",
    "y_mean, y_std = {}, {}\n",
    "for k in y_targets:\n",
    "    y_targets[k] = torch.nan_to_num(y_targets[k], nan=0.0)\n",
    "\n",
    "    y_mean[k] = y_targets[k].mean()\n",
    "    y_std[k] = y_targets[k].std()\n",
    "\n",
    "    if y_std[k] < 1e-5:\n",
    "        print(f\"âš ï¸ Skipping normalization for {k} (stdâ‰ˆ0)\")\n",
    "        y_targets[k] = y_targets[k] - y_mean[k]\n",
    "    else:\n",
    "        y_targets[k] = (y_targets[k] - y_mean[k]) / (y_std[k] + 1e-6)\n",
    "\n",
    "print(\"Targets normalized.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Initialize model\n",
    "# -------------------------------\n",
    "model = SanctionImpactGNN(\n",
    "    in_dim=graphs_dev[0].x.shape[1],\n",
    "    india_index=0\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Optimizer & loss\n",
    "# -------------------------------\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Optional: economic importance weights\n",
    "loss_weights = {\n",
    "    \"gdp\": 2.0,\n",
    "    \"cpi\": 2.0,\n",
    "    \"fx\": 1.5,\n",
    "    \"trade\": 1.0,\n",
    "    \"fdi\": 1.0,\n",
    "    \"res\": 1.0,\n",
    "    \"score\": 1.5,\n",
    "    \"duration\": 1.0\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Training loop\n",
    "# -------------------------------\n",
    "EPOCHS = 300\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    preds = model(graphs_dev)   # dict: key -> [T, 1]\n",
    "\n",
    "    # Multi-task weighted loss\n",
    "    loss = 0.0\n",
    "    for k in y_targets:\n",
    "        loss += loss_weights[k] * loss_fn(preds[k], y_targets[k])\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Logging\n",
    "    if epoch % 25 == 0 or epoch == EPOCHS - 1:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb5a4b16-7db0-4e3a-a7ad-a92a904f35ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdp mean: 1.668930025289228e-07 std: 0.9999997019767761 nan count: 0\n",
      "cpi mean: -5.9604645663569045e-09 std: 0.9999996423721313 nan count: 0\n",
      "fx mean: 0.0 std: 0.0 nan count: 0\n",
      "trade mean: -6.407499597571586e-08 std: 1.0 nan count: 0\n",
      "fdi mean: 2.9802322387695312e-08 std: 0.9999999403953552 nan count: 0\n",
      "res mean: -2.3841858265427618e-08 std: 1.0 nan count: 0\n",
      "score mean: 3.0249356086642365e-07 std: 1.0 nan count: 0\n",
      "duration mean: 0.0 std: 0.0 nan count: 0\n"
     ]
    }
   ],
   "source": [
    "for k, v in y_targets.items():\n",
    "    print(\n",
    "        k,\n",
    "        \"mean:\", v.mean().item(),\n",
    "        \"std:\", v.std().item(),\n",
    "        \"nan count:\", torch.isnan(v).sum().item()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d96a239-61a2-4b68-990a-4f48d20b4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_norm = model(graphs_dev)\n",
    "\n",
    "preds_real = {}\n",
    "for k in preds_norm:\n",
    "    preds_real[k] = preds_norm[k] * y_std[k] + y_mean[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "827c40ca-1d86-45d7-861d-e9958735b0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec8436ce69f4f1ead3f72cb082042f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FloatSlider(value=0.5, description='Severity', max=1.0, step=0.05), HBox(children=(Checkbox(valâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# -------- CONFIG --------\n",
    "india_index = 0        # same as model\n",
    "base_feature_dim = 10  # macro features already in data.x\n",
    "policy_dim = 7         # number of NLP policy signals\n",
    "\n",
    "# -------- WIDGETS --------\n",
    "severity = widgets.FloatSlider(\n",
    "    value=0.5, min=0.0, max=1.0, step=0.05,\n",
    "    description=\"Severity\"\n",
    ")\n",
    "\n",
    "financial = widgets.Checkbox(description=\"Financial\")\n",
    "trade = widgets.Checkbox(description=\"Trade\")\n",
    "technology = widgets.Checkbox(description=\"Technology\")\n",
    "energy = widgets.Checkbox(description=\"Energy\")\n",
    "\n",
    "issuer_strength = widgets.FloatSlider(\n",
    "    value=0.7, min=0.0, max=1.0, step=0.05,\n",
    "    description=\"Issuer Power\"\n",
    ")\n",
    "\n",
    "binding = widgets.Checkbox(description=\"Legally Binding\")\n",
    "\n",
    "apply_btn = widgets.Button(\n",
    "    description=\"Apply Policy Signal\",\n",
    "    button_style=\"success\"\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# -------- CALLBACK --------\n",
    "def apply_policy(_):\n",
    "    with output:\n",
    "        clear_output()\n",
    "\n",
    "        policy_vector = torch.tensor([\n",
    "            severity.value,\n",
    "            float(financial.value),\n",
    "            float(trade.value),\n",
    "            float(technology.value),\n",
    "            float(energy.value),\n",
    "            issuer_strength.value,\n",
    "            float(binding.value)\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        print(\"ðŸ“Œ Policy Signal Vector:\")\n",
    "        print(policy_vector)\n",
    "        print(\"\\nShape:\", policy_vector.shape)\n",
    "\n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(f\"Severity: {policy_vector[0]:.2f}\")\n",
    "        print(f\"Types: Financial={financial.value}, Trade={trade.value}, \"\n",
    "              f\"Tech={technology.value}, Energy={energy.value}\")\n",
    "        print(f\"Issuer strength: {policy_vector[5]:.2f}\")\n",
    "        print(f\"Binding: {binding.value}\")\n",
    "\n",
    "apply_btn.on_click(apply_policy)\n",
    "\n",
    "# -------- DISPLAY --------\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        severity,\n",
    "        widgets.HBox([financial, trade, technology, energy]),\n",
    "        issuer_strength,\n",
    "        binding,\n",
    "        apply_btn,\n",
    "        output\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00006a7d-8a1d-4a43-9ddf-284f6db2bd53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
